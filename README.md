# Toxic-Comment-Classification
A toxicity detection model was developed with the capability to identify six distinct types of toxicity: toxic, severe toxic, threats, obscenity, insults, and hate. The process involved various steps, including tokenization, removal of stopwords and punctuation using NLTK, lemmatization, and encoding through OneHotEncoding. The model's performance was enhanced by implementing an Embedding technique and a Bidirectional LSTM RNN network. This comprehensive approach yielded remarkable results, as evidenced by an impressive average accuracy of 95.92% across all models.
